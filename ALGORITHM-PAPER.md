# ATLAS: 会話における重要発話検出アルゴリズム（論文向け詳細説明）

## 概要

本文書では、ATLAS（Attention Temporal Link Analysis System）の重要発話検出アルゴリズムを、具体的な会話例と計算過程を用いて詳細に説明します。

---

## 1. 問題設定と基本原理

### 1.1 問題定義

**入力:**
- 会話履歴 $H = \{u_1, u_2, \ldots, u_{n-1}\}$（過去の発話列）
- 現在発話 $u_n$（新しく入力された発話）

**出力:**
- 重要発話集合 $I \subseteq H$（現在発話に対して重要な過去発話）

### 1.2 基本原理

会話履歴 $H$ から現在発話 $u_n$ を予測するタスクを考える。このとき、**ある発話 $u_i$ を除いた履歴で予測を行うと、予測精度が大きく低下する場合、その発話 $u_i$ は重要である**という仮説に基づく。

形式的には、以下の**損失差分**（delta loss）で重要度を測る:

$$
\Delta L(u_i) = L(H \setminus \{u_i\}, u_n) - L(H, u_n)
$$

ここで:
- $L(H, u_n)$: 履歴 $H$ から $u_n$ を予測する際の損失（低いほど予測が容易）
- $L(H \setminus \{u_i\}, u_n)$: $u_i$ を除いた履歴で $u_n$ を予測する際の損失

$\Delta L(u_i) > 0$ が大きいほど、$u_i$ は $u_n$ にとって重要。

---

## 2. アルゴリズムの全体構成

### 2.1 アルゴリズムのフローチャート

```
┌─────────────────────────────────────┐
│ 1. 候補選択                          │
│    直近 k 個の発話を評価対象とする   │
└────────────┬────────────────────────┘
             │
             ▼
┌─────────────────────────────────────┐
│ 2. 基準損失計算                      │
│    L_base = L(H, u_n)               │
└────────────┬────────────────────────┘
             │
             ▼
┌─────────────────────────────────────┐
│ 3. スコア計算（各候補に対して）       │
│    a) L_masked = L(H \ {u_i}, u_n) │
│    b) ΔL = L_masked - L_base       │
│    c) ω = exp(-λ × distance)       │
│    d) score = ΔL × ω               │
└────────────┬────────────────────────┘
             │
             ▼
┌─────────────────────────────────────┐
│ 4. 帰無分布生成                      │
│    履歴をランダムシャッフルして       │
│    ランダムな発話除去のスコア分布生成 │
└────────────┬────────────────────────┘
             │
             ▼
┌─────────────────────────────────────┐
│ 5. 統計的正規化                      │
│    z = 0.6745 × (score - med) / MAD│
└────────────┬────────────────────────┘
             │
             ▼
┌─────────────────────────────────────┐
│ 6. 重要発話判定                      │
│    I = {u_i | z_i > τ}             │
│    (τ = 1.64: 上位5%相当)           │
└─────────────────────────────────────┘
```

### 2.2 疑似コード

```python
def detect_important_utterances(history: List[Utterance],
                                current: Utterance,
                                k: int = 15,
                                half_life: int = 10,
                                null_samples: int = 20,
                                z_threshold: float = 1.64) -> List[Utterance]:
    """
    重要発話検出アルゴリズム

    Args:
        history: 会話履歴
        current: 現在発話
        k: 評価する候補数（直近k個）
        half_life: 時間減衰の半減期（ターン数）
        null_samples: 帰無分布のサンプル数
        z_threshold: 重要判定のz値閾値

    Returns:
        重要発話のリスト（z値の降順）
    """

    # 1. 候補選択: 直近k個
    candidates = history[-k:]

    # 2. 基準損失計算
    L_base = loss_with_history(history, current)

    # 3. 各候補のスコア計算
    scores = []
    for i, u_i in enumerate(candidates):
        # a) マスク損失
        masked_history = [u for u in history if u.id != u_i.id]
        L_masked = loss_with_history(masked_history, current)

        # b) 損失差分
        delta_loss = L_masked - L_base

        # c) 時間減衰重み
        distance = len(history) - history.index(u_i)
        lambda_decay = log(2) / half_life
        age_weight = exp(-lambda_decay * distance)

        # d) 最終スコア
        final_score = delta_loss * age_weight

        scores.append({
            'utterance': u_i,
            'score': final_score,
            'delta_loss': delta_loss,
            'age_weight': age_weight
        })

    # 4. 帰無分布生成
    null_scores = []
    for _ in range(max(null_samples, k * 3)):
        # ランダムシャッフル
        shuffled = random.shuffle(history.copy())
        L_base_null = loss_with_history(shuffled, current)

        # ランダムに1発話を除去
        random_utt = random.choice(shuffled)
        masked = [u for u in shuffled if u.id != random_utt.id]
        L_masked_null = loss_with_history(masked, current)

        null_scores.append(L_masked_null - L_base_null)

    # 5. ロバストz値で正規化
    all_scores = [s['score'] for s in scores] + null_scores
    z_scores = robust_z_normalize(all_scores)

    # スコアにz値を追加
    for i, score_obj in enumerate(scores):
        score_obj['z'] = z_scores[i]

    # 6. 重要発話判定（z > τ）
    important = [s for s in scores if s['z'] > z_threshold]
    important.sort(key=lambda x: x['z'], reverse=True)

    return important


def robust_z_normalize(values: List[float]) -> List[float]:
    """
    ロバストz値正規化（外れ値に強い）

    Args:
        values: スコアのリスト

    Returns:
        z値のリスト
    """
    med = median(values)
    deviations = [abs(v - med) for v in values]
    mad = median(deviations)

    # MADが小さすぎる場合の安定化
    mad_stable = max(mad, 0.001)

    # z値変換（0.6745は正規分布におけるMAD→標準偏差の変換係数）
    z_scores = [0.6745 * (v - med) / mad_stable for v in values]

    return z_scores
```

---

## 3. 具体例による計算過程

### 3.1 サンプル会話

以下のような会議の会話を考える:

| ID | 話者 | 発話内容 | タイムスタンプ |
|----|------|----------|---------------|
| 1 | Alice | 今日のミーティングの議題は何ですか？ | t=0 |
| 2 | Bob | プロジェクトの進捗確認とリスク管理です | t=10 |
| 3 | Alice | リスク管理について詳しく聞きたいです | t=25 |
| 4 | Bob | 主に予算超過のリスクがあります | t=40 |
| 5 | Charlie | 予算は現在どのくらい使っていますか？ | t=55 |
| 6 | Bob | 約70%消化しています | t=70 |
| 7 | Alice | それは問題ですね | t=80 |
| 8 | Charlie | 対策を考えましょう | t=90 |
| **9** | **Bob** | **コスト削減案をいくつか用意しました** | **t=100** |

**現在発話**: $u_9 =$ "コスト削減案をいくつか用意しました"

**目的**: $u_9$ に対して、過去の発話 $u_1, \ldots, u_8$ のうちどれが重要かを検出する。

### 3.2 ステップ1: 候補選択

パラメータ $k = 8$ とすると、全ての過去発話が候補となる。

$$
C = \{u_1, u_2, u_3, u_4, u_5, u_6, u_7, u_8\}
$$

### 3.3 ステップ2: 基準損失計算

まず、全履歴から現在発話を予測する際の損失を計算する。

#### 埋め込み表現の取得

各発話をOpenAI text-embedding-3-small（1536次元）で埋め込み表現に変換:

$$
\mathbf{e}_i = \text{Embed}(u_i) \in \mathbb{R}^{1536}
$$

#### 履歴の平均埋め込み

履歴全体の平均ベクトルを計算:

$$
\bar{\mathbf{e}}_H = \frac{1}{8} \sum_{i=1}^{8} \mathbf{e}_i
$$

#### コサイン類似度と損失

現在発話の埋め込み $\mathbf{e}_9$ と履歴平均 $\bar{\mathbf{e}}_H$ のコサイン類似度:

$$
\text{sim}(H, u_9) = \frac{\bar{\mathbf{e}}_H \cdot \mathbf{e}_9}{\|\bar{\mathbf{e}}_H\| \|\mathbf{e}_9\|}
$$

基準損失（1 - 類似度）:

$$
L_{\text{base}} = 1 - \text{sim}(H, u_9)
$$

**数値例（仮の値）:**
- $\text{sim}(H, u_9) = 0.65$
- $L_{\text{base}} = 1 - 0.65 = 0.35$

### 3.4 ステップ3: 各候補のスコア計算

各候補発話 $u_i$ について、以下を計算する。

#### 候補 $u_2$: "プロジェクトの進捗確認とリスク管理です"

**a) マスク損失:**

$u_2$ を除いた履歴で $u_9$ を予測:

$$
H \setminus \{u_2\} = \{u_1, u_3, u_4, u_5, u_6, u_7, u_8\}
$$

平均埋め込み:

$$
\bar{\mathbf{e}}_{H \setminus \{u_2\}} = \frac{1}{7} \sum_{i \in \{1,3,4,5,6,7,8\}} \mathbf{e}_i
$$

類似度:

$$
\text{sim}(H \setminus \{u_2\}, u_9) = \frac{\bar{\mathbf{e}}_{H \setminus \{u_2\}} \cdot \mathbf{e}_9}{\|\bar{\mathbf{e}}_{H \setminus \{u_2\}}\| \|\mathbf{e}_9\|}
$$

マスク損失:

$$
L_{\text{masked}}(u_2) = 1 - \text{sim}(H \setminus \{u_2\}, u_9)
$$

**数値例:**
- $\text{sim}(H \setminus \{u_2\}, u_9) = 0.58$（$u_2$ を除くと類似度が下がる）
- $L_{\text{masked}}(u_2) = 1 - 0.58 = 0.42$

**b) 損失差分:**

$$
\Delta L(u_2) = L_{\text{masked}}(u_2) - L_{\text{base}} = 0.42 - 0.35 = 0.07
$$

$u_2$ を除くと損失が0.07増加 → $u_2$ は重要な可能性

**c) 時間減衰重み:**

$u_2$ の位置: 8個前（最新が $u_9$、$u_2$ は index 2）

距離（現在からのターン数）:

$$
d = 9 - 2 = 7
$$

半減期 $T_{1/2} = 10$ ターンとすると、減衰定数:

$$
\lambda = \frac{\ln 2}{T_{1/2}} = \frac{0.693}{10} = 0.0693
$$

時間減衰重み:

$$
\omega(u_2) = e^{-\lambda \times 7} = e^{-0.0693 \times 7} = e^{-0.485} \approx 0.616
$$

**d) 最終スコア:**

$$
s(u_2) = \Delta L(u_2) \times \omega(u_2) = 0.07 \times 0.616 \approx 0.043
$$

---

#### 候補 $u_4$: "主に予算超過のリスクがあります"

**a) マスク損失:**

$$
H \setminus \{u_4\} = \{u_1, u_2, u_3, u_5, u_6, u_7, u_8\}
$$

**数値例:**
- $\text{sim}(H \setminus \{u_4\}, u_9) = 0.55$（$u_4$ を除くとさらに類似度が下がる）
- $L_{\text{masked}}(u_4) = 1 - 0.55 = 0.45$

**b) 損失差分:**

$$
\Delta L(u_4) = 0.45 - 0.35 = 0.10
$$

**c) 時間減衰重み:**

距離: $d = 9 - 4 = 5$

$$
\omega(u_4) = e^{-0.0693 \times 5} = e^{-0.347} \approx 0.707
$$

**d) 最終スコア:**

$$
s(u_4) = 0.10 \times 0.707 \approx 0.071
$$

---

#### 候補 $u_6$: "約70%消化しています"

**a) マスク損失:**

**数値例:**
- $\text{sim}(H \setminus \{u_6\}, u_9) = 0.59$
- $L_{\text{masked}}(u_6) = 1 - 0.59 = 0.41$

**b) 損失差分:**

$$
\Delta L(u_6) = 0.41 - 0.35 = 0.06
$$

**c) 時間減衰重み:**

距離: $d = 9 - 6 = 3$

$$
\omega(u_6) = e^{-0.0693 \times 3} = e^{-0.208} \approx 0.812
$$

**d) 最終スコア:**

$$
s(u_6) = 0.06 \times 0.812 \approx 0.049
$$

---

#### 候補 $u_7$: "それは問題ですね"

**数値例:**
- $L_{\text{masked}}(u_7) = 0.36$
- $\Delta L(u_7) = 0.36 - 0.35 = 0.01$（ほとんど影響なし）
- $d = 2$, $\omega(u_7) = 0.871$
- $s(u_7) = 0.01 \times 0.871 \approx 0.009$

---

#### 全候補のスコア（仮の値）

| 発話ID | 内容 | $\Delta L$ | $\omega$ | スコア $s$ |
|-------|------|-----------|---------|-----------|
| 1 | 今日のミーティングの議題は何ですか？ | 0.02 | 0.523 | 0.010 |
| 2 | プロジェクトの進捗確認とリスク管理です | 0.07 | 0.616 | **0.043** |
| 3 | リスク管理について詳しく聞きたいです | 0.04 | 0.656 | 0.026 |
| 4 | 主に予算超過のリスクがあります | 0.10 | 0.707 | **0.071** |
| 5 | 予算は現在どのくらい使っていますか？ | 0.05 | 0.760 | 0.038 |
| 6 | 約70%消化しています | 0.06 | 0.812 | **0.049** |
| 7 | それは問題ですね | 0.01 | 0.871 | 0.009 |
| 8 | 対策を考えましょう | 0.08 | 0.933 | **0.075** |

### 3.5 ステップ4: 帰無分布生成

統計的有意性を評価するため、ランダムな発話除去でのスコア分布を生成する。

**手順:**
1. 履歴 $H$ をランダムにシャッフル
2. シャッフルした履歴で基準損失を計算
3. ランダムに1発話を除いてマスク損失を計算
4. 差分スコアを記録
5. これを $n_{\text{null}} = \max(20, k \times 3) = 24$ 回繰り返す

**帰無分布のサンプル（仮の値）:**

```
null_scores = [0.005, 0.012, -0.003, 0.008, 0.015, 0.002, 0.018, 0.010,
               0.007, 0.020, 0.009, -0.001, 0.013, 0.006, 0.011, 0.014,
               0.004, 0.016, 0.008, 0.012, 0.007, 0.019, 0.010, 0.005]
```

帰無分布のスコアは、ランダムに除去した場合の「偶然のスコア」を表す。

### 3.6 ステップ5: 統計的正規化（ロバストz値）

全スコア（候補スコア + 帰無スコア）をロバストz値で正規化する。

**全スコア:**

```
all_scores = [0.010, 0.043, 0.026, 0.071, 0.038, 0.049, 0.009, 0.075,  // 候補8個
              0.005, 0.012, -0.003, ..., 0.005]  // 帰無24個
total = 32個
```

**中央値（Median）:**

$$
\text{med} = \text{median}(\text{all\_scores}) \approx 0.011
$$

**絶対偏差:**

$$
\text{deviations} = [|0.010 - 0.011|, |0.043 - 0.011|, \ldots] = [0.001, 0.032, 0.015, 0.060, \ldots]
$$

**MAD（Median Absolute Deviation）:**

$$
\text{MAD} = \text{median}(\text{deviations}) \approx 0.008
$$

**安定化:**

$$
\text{MAD}_{\text{stable}} = \max(0.008, 0.001) = 0.008
$$

**z値計算（候補のみ）:**

$$
z_i = \frac{0.6745 \times (s_i - \text{med})}{\text{MAD}_{\text{stable}}}
$$

| 発話ID | スコア $s$ | $s - \text{med}$ | z値 |
|-------|-----------|-----------------|-----|
| 1 | 0.010 | -0.001 | -0.08 |
| 2 | 0.043 | 0.032 | **2.69** |
| 3 | 0.026 | 0.015 | 1.26 |
| 4 | 0.071 | 0.060 | **5.06** |
| 5 | 0.038 | 0.027 | 2.28 |
| 6 | 0.049 | 0.038 | **3.20** |
| 7 | 0.009 | -0.002 | -0.17 |
| 8 | 0.075 | 0.064 | **5.40** |

### 3.7 ステップ6: 重要発話判定

閾値 $\tau = 1.64$（上位5%相当）を使用。

**重要発話（$z > 1.64$）:**

| ランク | 発話ID | 内容 | z値 |
|-------|-------|------|-----|
| 1 | 8 | 対策を考えましょう | 5.40 |
| 2 | 4 | 主に予算超過のリスクがあります | 5.06 |
| 3 | 6 | 約70%消化しています | 3.20 |
| 4 | 2 | プロジェクトの進捗確認とリスク管理です | 2.69 |
| 5 | 5 | 予算は現在どのくらい使っていますか？ | 2.28 |

**解釈:**

現在発話「コスト削減案をいくつか用意しました」に対して:
- **最も重要**: $u_8$ "対策を考えましょう"（直前の文脈）
- **次に重要**: $u_4$ "主に予算超過のリスクがあります"（問題の核心）
- **同様に重要**: $u_6$ "約70%消化しています"（具体的な状況）、$u_2$ "プロジェクトの進捗確認とリスク管理です"（トピック導入）

これらの発話が除外されると、「コスト削減案」という現在発話を予測することが困難になる。

---

## 4. アルゴリズムの特徴と利点

### 4.1 時間減衰の効果

時間減衰関数 $\omega(d) = e^{-\lambda d}$ により、以下の特性を実現:

**例: 半減期10ターン**

| 距離（ターン前） | 重み $\omega$ | 割合 |
|----------------|-------------|------|
| 0（直前） | 1.000 | 100% |
| 5 | 0.707 | 71% |
| 10 | 0.500 | 50% |
| 20 | 0.250 | 25% |
| 30 | 0.125 | 13% |

**利点:**
- 直近の文脈を重視しつつ、遠い過去の重要発話も適度に考慮
- 短い会話（10ターン以内）では、全発話が比較的均等に評価される
- 長い会話（100ターン以上）では、古い発話の影響を自動的に減衰

### 4.2 ロバストz値の利点

**標準z値との比較:**

標準z値: $z = \frac{x - \mu}{\sigma}$（平均と標準偏差）

ロバストz値: $z = \frac{0.6745 \times (x - \text{med})}{\text{MAD}}$（中央値とMAD）

**利点:**
- **外れ値に強い**: 極端なスコアの影響を受けにくい
- **短いサンプルでも安定**: 候補数が少ない（k=5-15）場合でも安定した正規化
- **分布の形状に依存しない**: 正規分布を仮定しない

**数値例:**

以下のスコア分布を考える:

```
scores = [0.01, 0.02, 0.03, 0.04, 0.05, 0.50]  // 0.50は外れ値
```

| 手法 | 平均/中央値 | 標準偏差/MAD | 0.05のz値 |
|-----|-----------|------------|-----------|
| 標準z値 | 0.108 | 0.187 | -0.31（低評価） |
| ロバストz値 | 0.035 | 0.015 | **1.01**（高評価） |

外れ値0.50の影響で、標準z値では0.05が過小評価されるが、ロバストz値では適切に評価される。

### 4.3 帰無分布による統計的検定

**目的:** ランダムな発話除去でも高スコアが出る可能性を考慮

**効果:**

帰無分布のスコアが $[0.005, 0.020]$ の範囲にある場合:
- 候補スコア0.075（帰無分布の上位1%程度）→ 統計的に有意
- 候補スコア0.015（帰無分布の中央値程度）→ 統計的に有意でない

これにより、「たまたま高スコア」になった発話を除外し、真に重要な発話のみを検出。

---

## 5. パラメータの選択根拠

### 5.1 候補数 k = 15

**根拠:**
- 短い会話（10-20ターン）: 全履歴を評価可能
- 中程度の会話（50-100ターン）: 直近の文脈を中心に評価（遠い過去は時間減衰で自動調整）
- 長い会話（100ターン以上）: 計算量を抑えつつ、重要な文脈をカバー

**感度分析:**

| k | 利点 | 欠点 |
|---|------|------|
| 5 | 高速、直近重視 | 重要な過去発話を見逃す可能性 |
| 15 | バランスが良い | - |
| 30 | 長期的な文脈を考慮 | 計算量増大、ノイズ増加 |

### 5.2 半減期 $T_{1/2}$ = 10ターン

**根拠:**
- 会話の自然な文脈の範囲: 通常、5-15ターン前までの発話が現在に影響
- 短すぎる（5ターン）: トピック導入発話を見逃す
- 長すぎる（50ターン）: 無関係な古い発話が高スコアになる

**会話例での効果:**

```
[10ターン前] トピックA導入 (ω = 0.5) → 50%の重みで考慮
[5ターン前] トピックA詳細 (ω = 0.71) → 71%の重みで考慮
[直前] トピックA関連質問 (ω = 0.93) → 93%の重みで考慮
→ 現在: トピックAへの回答
```

トピック導入発話も適度に考慮されつつ、直近の質問が強く影響する。

### 5.3 z閾値 $\tau$ = 1.64（上位5%）

**根拠:**
- 統計的有意水準: p < 0.05 に対応（片側検定）
- 実用的な検出率: 会話20ターンで1-3個の重要発話を検出（適度な通知頻度）

**閾値による検出率の違い:**

| $\tau$ | 検出率（正規分布仮定） | 実用的な意味 |
|--------|---------------------|------------|
| 1.0 | 上位16% | 多くの発話が重要判定（通知過多） |
| 1.28 | 上位10% | やや緩い基準 |
| 1.64 | 上位5% | バランスが良い |
| 2.0 | 上位2.3% | 厳しい基準（見逃しリスク） |

### 5.4 帰無サンプル数 = max(20, k × 3)

**根拠:**
- 統計的安定性: 中央値とMADの安定した推定には最低20サンプル必要
- 候補との比率: 候補数の3倍以上で、帰無分布が候補分布を「圧倒」しないように調整

**数値例:**

- k = 15の場合: 帰無サンプル = max(20, 45) = 45個
- 全体32個中、候補15個（47%）vs 帰無45個（53%）
- 候補の極端なスコアが適切に検出される

---

## 6. 計算量とスケーラビリティ

### 6.1 時間計算量

**埋め込み取得:**
- 候補数: $k = 15$
- 帰無サンプル: $n_{\text{null}} = 45$
- 各サンプルで2回の埋め込み（全履歴平均 + マスク履歴平均）

**総埋め込み呼び出し数:**

$$
N_{\text{embed}} \approx k \times 2 + n_{\text{null}} \times 2 = 15 \times 2 + 45 \times 2 = 120
$$

**レイテンシ（OpenAI API）:**
- 1埋め込み: 約20ms（バッチリクエスト時）
- 総時間: $120 \times 20\text{ms} = 2400\text{ms} \approx 2.4\text{秒}$

**並列化:**
- 候補スコア計算と帰無分布生成を並列実行
- 実際のレイテンシ: 約1.5-2秒

### 6.2 空間計算量

**埋め込みベクトル:**
- 1発話: 1536次元 × 4バイト（float32） = 6,144バイト ≈ 6KB
- 会話100ターン: 600KB
- アンカーメモリ200個: 1.2MB

**総メモリ使用量:**
- 典型的なセッション（100ターン）: 約2-3MB（許容範囲）

### 6.3 スケーラビリティ

**会話長に対する挙動:**

| 会話長 | 候補数 | 埋め込み呼び出し | レイテンシ | 備考 |
|-------|-------|---------------|----------|------|
| 10ターン | 10 | 80 | 1.5秒 | 全履歴を評価 |
| 50ターン | 15 | 120 | 2秒 | 直近15個を評価 |
| 100ターン | 15 | 120 | 2秒 | 変わらず |
| 500ターン | 15 | 120 | 2秒 | 変わらず（時間減衰で古い発話の影響小） |

**利点:**
- $k$ を固定することで、会話が長くなっても計算量は一定
- 時間減衰により、古い発話は自動的に重要度が下がる

---

## 7. アルゴリズムの評価と妥当性

### 7.1 直感的な解釈

**"予測損失の増加" = 重要度** の意味:

1. **意味的な関連性**: 現在発話と意味的に近い過去発話は、その発話がないと現在発話を予測しにくい
2. **文脈の継続性**: トピックを導入した発話や、議論の核心となる発話は、その後の発話を理解する上で不可欠
3. **情報の補完性**: 具体的な数値や固有名詞を含む発話は、抽象的な発話を理解するために必要

### 7.2 具体例での妥当性

前述の会話例で:
- 現在発話「コスト削減案をいくつか用意しました」
- 重要発話:
  - "対策を考えましょう"（直前の文脈）
  - "主に予算超過のリスクがあります"（問題の核心）
  - "約70%消化しています"（具体的な状況）

これらは人間が直感的に「重要」と感じる発話と一致している。

### 7.3 比較手法との違い

| 手法 | 原理 | 利点 | 欠点 |
|-----|------|------|------|
| **キーワードマッチ** | 特定の単語の出現 | 高速、簡単 | 意味的な関連性を捉えられない |
| **TF-IDF** | 単語の重要度 | 計算が軽い | 文脈を考慮しない |
| **BERTベースの類似度** | 発話間の意味的類似度 | 意味を捉える | 「除外時の影響」を評価しない |
| **本手法（マスクベース）** | 除外時の予測損失 | 文脈における重要性を直接測定 | 計算量がやや大きい |

### 7.4 実験的な妥当性（期待される結果）

**期待される検出パターン:**

1. **トピック導入発話**: 新しい議題を提示する発話は、その後の発話に強く影響
2. **質問と回答のペア**: 質問に対する回答は、質問を文脈として必要とする
3. **具体的な情報**: 数値、固有名詞、専門用語を含む発話は、抽象的な議論を補完
4. **意思決定に関わる発話**: 合意、決定、アクションアイテムは後続の発話に影響

**評価指標:**
- **適合率**: 検出された重要発話のうち、人間が重要と判断した割合
- **再現率**: 人間が重要と判断した発話のうち、検出された割合
- **F1スコア**: 適合率と再現率の調和平均

---

## 8. 結論

本アルゴリズムは、**マスクベースの予測損失**という明確な原理に基づき、会話における重要発話を自動検出する。具体的な計算過程を示すことで、以下の特徴が明らかになった:

1. **意味的な関連性**: 埋め込み空間での類似度により、意味的に関連する発話を検出
2. **時間的な重み付け**: 指数減衰により、直近の文脈を重視しつつ、重要な過去発話も考慮
3. **統計的な信頼性**: ロバストz値と帰無分布により、偶然の高スコアを排除
4. **計算効率**: 候補数を制限し、並列処理により、実用的なレイテンシを実現

今後の研究では、実際の会話データでの評価実験、パラメータの最適化、および他の手法との比較を通じて、本アルゴリズムの有効性をさらに検証する予定である。

---

## 付録A: 数学的な記法一覧

| 記号 | 意味 |
|------|------|
| $H$ | 会話履歴（発話の集合） |
| $u_i$ | $i$ 番目の発話 |
| $u_n$ | 現在発話 |
| $L(H, u_n)$ | 履歴 $H$ から $u_n$ を予測する際の損失 |
| $\Delta L(u_i)$ | 発話 $u_i$ の損失差分（重要度の生値） |
| $\omega(u_i)$ | 発話 $u_i$ の時間減衰重み |
| $s(u_i)$ | 発話 $u_i$ の最終スコア |
| $z_i$ | 発話 $u_i$ のロバストz値 |
| $k$ | 候補数 |
| $T_{1/2}$ | 時間減衰の半減期（ターン数） |
| $\lambda$ | 減衰定数 = $\ln 2 / T_{1/2}$ |
| $\tau$ | 重要判定のz値閾値 |
| $\mathbf{e}_i$ | 発話 $u_i$ の埋め込みベクトル（1536次元） |
| $\text{med}$ | 中央値 |
| $\text{MAD}$ | Median Absolute Deviation（絶対偏差の中央値） |

---

## 付録B: 実装上の注意点

### B.1 空文字列のハンドリング

OpenAI埋め込みAPIは空文字列を拒否するため、事前にバリデーションが必要:

```typescript
if (!text || text.trim().length === 0) {
  throw new Error('Empty text cannot be embedded');
}
```

### B.2 MADの安定化

MADが0に近い場合（すべてのスコアがほぼ同じ）、z値が発散する。最小値0.001で安定化:

```typescript
const madStable = Math.max(mad, 0.001);
```

### B.3 並列処理のエラーハンドリング

埋め込みAPIの呼び出しが失敗した場合、一部の候補をスキップして処理を継続:

```typescript
const results = await Promise.allSettled(
  candidates.map(c => scoreUtterance(c))
);

const successful = results
  .filter(r => r.status === 'fulfilled')
  .map(r => r.value);
```

### B.4 キャッシュの活用

同じ発話に対する埋め込みを再計算しないよう、メモリキャッシュを使用:

```typescript
const cache = new Map<string, number[]>();

async function embed(text: string): Promise<number[]> {
  if (cache.has(text)) return cache.get(text)!;

  const embedding = await api.embed(text);
  cache.set(text, embedding);
  return embedding;
}
```

---

## 参考文献

1. Devlin et al. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
2. Reimers & Gurevych (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
3. OpenAI (2024). "Text Embeddings API Documentation"
4. Rousseeuw & Croux (1993). "Alternatives to the Median Absolute Deviation"
5. Leys et al. (2013). "Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median"
